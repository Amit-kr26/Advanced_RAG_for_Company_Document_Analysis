# ├── src
#     │   ├── __init__.py
#     │   ├── api_request_parallel_processor.py
#     │   ├── api_requests.py
#     │   ├── ingestion.py
#     │   ├── parsed_reports_merging.py
#     │   ├── pdf_parsing.py
#     │   ├── pipeline.py
#     │   ├── prompts.py
#     │   ├── questions_processing.py
#     │   ├── reranking.py
#     │   ├── retrieval.py
#     │   ├── tables_serialization.py
#     │   └── text_splitter.py
#     ├── main.py


# since my code is too long, i will provide each file summary except prompts.py 

# init is empty
# prompts.py 
# from pydantic import BaseModel, Field
# from typing import Literal, List, Union
# import inspect
# import re


# def build_system_prompt(instruction: str="", example: str="", pydantic_schema: str="") -> str:
#     delimiter = "\n\n---\n\n"
#     schema = f"Your answer should be in JSON and strictly follow this schema, filling in the fields in the order they are given:\n```\n{pydantic_schema}\n```"
#     if example:
#         example = delimiter + example.strip()
#     if schema:
#         schema = delimiter + schema.strip()
    
#     system_prompt = instruction.strip() + schema + example
#     return system_prompt

# class RephrasedQuestionsPrompt:
#     instruction = """
# You are a question rephrasing system.
# Your task is to break down a comparative question into individual questions for each company mentioned.
# Each output question must be self-contained, maintain the same intent and metric as the original question, be specific to the respective company, and use consistent phrasing.
# """

#     class RephrasedQuestion(BaseModel):
#         """Individual question for a company"""
#         company_name: str = Field(description="Company name, exactly as provided in quotes in the original question")
#         question: str = Field(description="Rephrased question specific to this company")

#     class RephrasedQuestions(BaseModel):
#         """List of rephrased questions"""
#         questions: List['RephrasedQuestionsPrompt.RephrasedQuestion'] = Field(description="List of rephrased questions for each company")

#     pydantic_schema = '''
# class RephrasedQuestion(BaseModel):
#     """Individual question for a company"""
#     company_name: str = Field(description="Company name, exactly as provided in quotes in the original question")
#     question: str = Field(description="Rephrased question specific to this company")

# class RephrasedQuestions(BaseModel):
#     """List of rephrased questions"""
#     questions: List['RephrasedQuestionsPrompt.RephrasedQuestion'] = Field(description="List of rephrased questions for each company")
# '''

#     example = r"""
# Example:
# Input:
# Original comparative question: 'Which company had higher revenue in 2022, "Apple" or "Microsoft"?'
# Companies mentioned: "Apple", "Microsoft"

# Output:
# {
#     "questions": [
#         {
#             "company_name": "Apple",
#             "question": "What was Apple's revenue in 2022?"
#         },
#         {
#             "company_name": "Microsoft", 
#             "question": "What was Microsoft's revenue in 2022?"
#         }
#     ]
# }
# """

#     user_prompt = "Original comparative question: '{question}'\n\nCompanies mentioned: {companies}"

#     system_prompt = build_system_prompt(instruction, example)

#     system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)


# class AnswerWithRAGContextSharedPrompt:
#     instruction = """
# You are a RAG (Retrieval-Augmented Generation) answering system.
# Your task is to answer the given question based only on information from the company's annual report, which is uploaded in the format of relevant pages extracted using RAG.

# Before giving a final answer, carefully think out loud and step by step. Pay special attention to the wording of the question.
# - Keep in mind that the content containing the answer may be worded differently than the question.
# - The question was autogenerated from a template, so it may be meaningless or not applicable to the given company.
# """

#     user_prompt = """
# Here is the context:
# \"\"\"
# {context}
# \"\"\"

# ---

# Here is the question:
# "{question}"
# """

# class AnswerWithRAGContextNamePrompt:
#     instruction = AnswerWithRAGContextSharedPrompt.instruction
#     user_prompt = AnswerWithRAGContextSharedPrompt.user_prompt

#     class AnswerSchema(BaseModel):
#         step_by_step_analysis: str = Field(description="Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words. Pay special attention to the wording of the question to avoid being tricked. Sometimes it seems that there is an answer in the context, but this is might be not the requested value, but only a similar one.")

#         reasoning_summary: str = Field(description="Concise summary of the step-by-step reasoning process. Around 50 words.")

#         relevant_pages: List[int] = Field(description="""
# List of page numbers containing information directly used to answer the question. Include only:
# - Pages with direct answers or explicit statements
# - Pages with key information that strongly supports the answer
# Do not include pages with only tangentially related information or weak connections to the answer.
# At least one page should be included in the list.
# """)

#         final_answer: Union[str, Literal["N/A"]] = Field(description="""
# If it is a company name, should be extracted exactly as it appears in question.
# If it is a person name, it should be their full name.
# If it is a product name, it should be extracted exactly as it appears in the context.
# Without any extra information, words or comments.
# - Return 'N/A' if information is not available in the context
# """)

#     pydantic_schema = re.sub(r"^ {4}", "", inspect.getsource(AnswerSchema), flags=re.MULTILINE)

#     example = r"""
# Example:
# Question: 
# "Who was the CEO of 'Southwest Airlines Co.'?" 

# Answer: 
# ```
# {
#   "step_by_step_analysis": "1. The question asks for the CEO of 'Southwest Airlines Co.'. The CEO is typically the highest-ranking executive responsible for the overall management of the company, sometimes referred to as the President or Managing Director.\n2. My source of information is a document that appears to be 'Southwest Airlines Co.''s annual report. This document will be used to identify the individual holding the CEO position.\n3. Within the provided document, there is a section that identifies Robert E. Jordan as the President & Chief Executive Officer of 'Southwest Airlines Co.'. The document confirms his role since February 2022.\n4. Therefore, based on the information found in the document, the CEO of 'Southwest Airlines Co.' is Robert E. Jordan.",
#   "reasoning_summary": "'Southwest Airlines Co.''s annual report explicitly names Robert E. Jordan as President & Chief Executive Officer since February 2021. This directly answers the question.",
#   "relevant_pages": [58],
#   "final_answer": "Robert E. Jordan"
# }
# ```
# """ 

#     system_prompt = build_system_prompt(instruction, example)

#     system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)



# class AnswerWithRAGContextNumberPrompt:
#     instruction = AnswerWithRAGContextSharedPrompt.instruction
#     user_prompt = AnswerWithRAGContextSharedPrompt.user_prompt

#     class AnswerSchema(BaseModel):
#         step_by_step_analysis: str = Field(description="""
# Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words.
# **Strict Metric Matching Required:**    

# 1. Determine the precise concept the question's metric represents. What is it actually measuring?
# 2. Examine potential metrics in the context. Don't just compare names; consider what the context metric measures.
# 3. Accept ONLY if: The context metric's meaning *exactly* matches the target metric. Synonyms are acceptable; conceptual differences are NOT.
# 4. Reject (and use 'N/A') if:
#     - The context metric covers more or less than the question's metric.
#     - The context metric is a related concept but not the *exact* equivalent (e.g., a proxy or a broader category).
#     - Answering requires calculation, derivation, or inference.
#     - Aggregation Mismatch: The question needs a single value but the context offers only an aggregated total
# 5. No Guesswork: If any doubt exists about the metric's equivalence, default to `N/A`."
# """)

#         reasoning_summary: str = Field(description="Concise summary of the step-by-step reasoning process. Around 50 words.")

#         relevant_pages: List[int] = Field(description="""
# List of page numbers containing information directly used to answer the question. Include only:
# - Pages with direct answers or explicit statements
# - Pages with key information that strongly supports the answer
# Do not include pages with only tangentially related information or weak connections to the answer.
# At least one page should be included in the list.
# """)

#         final_answer: Union[float, int, Literal['N/A']] = Field(description="""
# An exact metric number is expected as the answer.
# - Example for percentages:
#     Value from context: 58,3%
#     Final answer: 58.3

# Pay special attention to any mentions in the context about whether metrics are reported in units, thousands, or millions to adjust number in final answer with no changes, three zeroes or six zeroes accordingly.
# Pay attention if value wrapped in parentheses, it means that value is negative.

# - Example for negative values:
#     Value from context: (2,124,837) CHF
#     Final answer: -2124837

# - Example for numbers in thousands:
#     Value from context: 4970,5 (in thousands $)
#     Final answer: 4970500

# - Return 'N/A' if metric provided is in a different currency than mentioned in the question
#     Example of value from context: 780000 USD, but question mentions EUR
#     Final answer: 'N/A'

# - Return 'N/A' if metric is not directly stated in context EVEN IF it could be calculated from other metrics in the context
#     Example: Requested metric: Dividend per Share; Only available metrics from context: Total Dividends Paid ($5,000,000), and Number of Outstanding Shares (1,000,000); Calculated DPS = Total Dividends / Outstanding Shares.
#     Final answer: 'N/A'

# - Return 'N/A' if information is not available in the context
# """)

#     pydantic_schema = re.sub(r"^ {4}", "", inspect.getsource(AnswerSchema), flags=re.MULTILINE)

#     example = r"""
# Example 1:
# Question:
# "What was the total assets of 'Waste Connections Inc.' in the fiscal year 2022?"

# Answer:
# ```
# {
#   "step_by_step_analysis": "1. **Metric Definition:** The question asks for 'total assets' for 'Waste Connections Inc.' in fiscal year 2022.  'Total assets' represents the sum of all resources owned or controlled by the company, expected to provide future economic benefits.\n2. **Context Examination:** The context includes 'Consolidated Balance Sheets' (page 78), a standard financial statement that reports a company's assets, liabilities, and equity.\n3. **Metric Matching:** On page 78, under 'December 31, 2022', a line item labeled 'Total assets' exists.  This directly matches the concept requested in the question.\n4. **Value Extraction and Adjustment:** The value for 'Total assets' is '$18,500,342'. The context indicates this is in thousands of dollars.  Therefore, the full value is 18,500,342,000.\n5. **Confirmation**: No calculation beyond unit adjustment was needed. The reported metric directly matches the question.",
#   "reasoning_summary": "The 'Total assets' value for fiscal year 2022 was directly found on the 'Consolidated Balance Sheets' (page 78). The reported value was in thousands, requiring multiplication by 1000 for the final answer.",
#   "relevant_pages": [78],
#   "final_answer": 18500342000
# }
# ```


# Example 2:
# Question:
# "For Ritter Pharmaceuticals, Inc., what was the value of Research and development equipment, at cost at the end of the period listed in annual report?"

# Answer:
# ```
# {
#   "step_by_step_analysis": "1. The question asks for 'Research and development equipment, at cost' for Ritter Pharmaceuticals, Inc. This indicates a specific value from the balance sheet, representing the *original purchase price* of equipment specifically used for R&D, *without* any accumulated depreciation.\n2. The context (page 35) shows 'Property and equipment, net' at $12,500.  This is a *net* value (after depreciation), and it's a *broader* category, encompassing all property and equipment, not just R&D equipment.\n3. The context (page 37) also mentions 'Accumulated Depreciation' of $110,000 for 'Machinery and Equipment'. This represents the total *depreciation*, not the original cost, and, importantly, it doesn't specify that this equipment is *exclusively* for R&D.\n4. Neither of these metrics *exactly* matches the requested metric. 'Property and equipment, net' is too broad and represents the depreciated value. 'Accumulated Depreciation' only shows depreciation, not cost, and lacks R&D specificity.\n5. Since the context doesn't provide the *original cost* of *only* R&D equipment, and we cannot make assumptions, perform calculations, or combine information, the answer is 'N/A'.",
#   "reasoning_summary": "The context lacks a specific line item for 'Research and development equipment, at cost.' 'Property and equipment, net' is depreciated and too broad, while 'Accumulated Depreciation' only represents depreciation, not original cost, and is not R&D-specific. Strict matching requires 'N/A'.",
#   "relevant_pages": [ 35, 37 ],
#   "final_answer": "N/A"
# }
# ```
# """

#     system_prompt = build_system_prompt(instruction, example)

#     system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)



# class AnswerWithRAGContextBooleanPrompt:
#     instruction = AnswerWithRAGContextSharedPrompt.instruction
#     user_prompt = AnswerWithRAGContextSharedPrompt.user_prompt

#     class AnswerSchema(BaseModel):
#         step_by_step_analysis: str = Field(description="Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words. Pay special attention to the wording of the question to avoid being tricked. Sometimes it seems that there is an answer in the context, but this is might be not the requested value, but only a similar one.")

#         reasoning_summary: str = Field(description="Concise summary of the step-by-step reasoning process. Around 50 words.")

#         relevant_pages: List[int] = Field(description="""
# List of page numbers containing information directly used to answer the question. Include only:
# - Pages with direct answers or explicit statements
# - Pages with key information that strongly supports the answer
# Do not include pages with only tangentially related information or weak connections to the answer.
# At least one page should be included in the list.
# """)
        
#         final_answer: Union[bool] = Field(description="""
# A boolean value (True or False) extracted from the context that precisely answers the question.
# If question ask about did something happen, and in context there is information about it, return False.
# """)

#     pydantic_schema = re.sub(r"^ {4}", "", inspect.getsource(AnswerSchema), flags=re.MULTILINE)

#     example = r"""
# Question:
# "Did W. P. Carey Inc. announce any changes to its dividend policy in the annual report?"

# Answer:
# ```
# {
#   "step_by_step_analysis": "1. The question asks whether W. P. Carey Inc. announced changes to its dividend policy.\n2. The phrase 'changes to its dividend policy' requires careful interpretation. It means any adjustment to the framework, rules, or stated intentions that dictate how the company determines and distributes dividends.\n3. The context (page 12, 18) states that the company increased its annualized dividend to $4.27 per share in the fourth quarter of 2023, compared to $4.22 per share in the same period of 2022. Page 45 mentions further details about dividend.\n4. Consistent, incremental increases throughout the year, with explicit mentions of maintaining a 'steady and growing' dividend, indicates no changes to *policy*, though the *amount* increased as planned within the existing policy.",
#   "reasoning_summary": "The context highlights consistent, small increases to the dividend throughout the year, consistent with a stated policy of providing a 'steady and growing' dividend. While the dividend *amount* changed, the *policy* governing those increases remained consistent. The question asks about *policy* changes, not amount changes.",
#   "relevant_pages": [12, 18, 45],
#   "final_answer": False
# }
# ```
# """

#     system_prompt = build_system_prompt(instruction, example)

#     system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)



# class AnswerWithRAGContextNamesPrompt:
#     instruction = AnswerWithRAGContextSharedPrompt.instruction
#     user_prompt = AnswerWithRAGContextSharedPrompt.user_prompt

#     class AnswerSchema(BaseModel):
#         step_by_step_analysis: str = Field(description="Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words. Pay special attention to the wording of the question to avoid being tricked. Sometimes it seems that there is an answer in the context, but this is might be not the requested entity, but only a similar one.")

#         reasoning_summary: str = Field(description="Concise summary of the step-by-step reasoning process. Around 50 words.")

#         relevant_pages: List[int] = Field(description="""
# List of page numbers containing information directly used to answer the question. Include only:
# - Pages with direct answers or explicit statements
# - Pages with key information that strongly supports the answer
# Do not include pages with only tangentially related information or weak connections to the answer.
# At least one page should be included in the list.
# """)

#         final_answer: Union[List[str], Literal["N/A"]] = Field(description="""
# Each entry should be extracted exactly as it appears in the context.

# If the question asks about positions (e.g., changes in positions), return ONLY position titles, WITHOUT names or any additional information. Appointments on new leadership positions also should be counted as changes in positions. If several changes related to position with same title are mentioned, return title of such position only once. Position title always should be in singular form.
# Example of answer ['Chief Technology Officer', 'Board Member', 'Chief Executive Officer']

# If the question asks about names, return ONLY the full names exactly as they are in the context.
# Example of answer ['Carly Kennedy', 'Brian Appelgate Jr.']

# If the question asks about new launched products, return ONLY the product names exactly as they are in the context. Candidates for new products or products in testing phase not counted as new launched products.
# Example of answer ['EcoSmart 2000', 'GreenTech Pro']

# - Return 'N/A' if information is not available in the context
# """)

#     pydantic_schema = re.sub(r"^ {4}", "", inspect.getsource(AnswerSchema), flags=re.MULTILINE)

#     example = r"""
# Example:
# Question:
# "What are the names of all new executives that took on new leadership positions in company?"

# Answer:
# ```
# {
#     "step_by_step_analysis": "1. The question asks for the names of all new executives who took on new leadership positions in the company.\n2. Exhibit 10.9 and 10.10, as listed in the Exhibit Index on page 89, mentions new Executive Agreements with Carly Kennedy and Brian Appelgate.\n3. Exhibit 10.9, Employment Agreement with Carly Kennedy, states her start date as April 4, 2022, and her position as Executive Vice President and General Counsel.\n4. Exhibit 10.10, Offer Letter with Brian Appelgate shows that his new role within the company is Interim Chief Operations Officer, and he was accepting the offer on November 8, 2022.\n5. Based on the documents, Carly Kennedy and Brian Appelgate are named as the new executives.",
#     "reasoning_summary": "Exhibits 10.9 and 10.10 of the annual report, described as Employment Agreement and Offer Letter, explicitly name Carly Kennedy and Brian Appelgate taking on new leadership roles within the company in 2022.",
#     "relevant_pages": [
#         89
#     ],
#     "final_answer": [
#         "Carly Kennedy",
#         "Brian Appelgate"
#     ]
# }
# ```
# """

#     system_prompt = build_system_prompt(instruction, example)

#     system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)

# class ComparativeAnswerPrompt:
#     instruction = """
# You are a question answering system.
# Your task is to analyze individual company answers and provide a comparative response that answers the original question.
# Base your analysis only on the provided individual answers - do not make assumptions or include external knowledge.
# Before giving a final answer, carefully think out loud and step by step.

# Important rules for comparison:
# - When the question asks to choose one of the companies (e.g., when comparing metrics), return the company name exactly as it appears in the original question
# - If a company's metric is in a different currency than what is asked in the question, exclude that company from comparison
# - If all companies are excluded (due to currency mismatch or other reasons), return 'N/A' as the final answer
# - If all companies except one are excluded, return the name of the remaining company (even though there is no actual comparison possible)
# """

#     user_prompt = """
# Here are the individual company answers:
# \"\"\"
# {context}
# \"\"\"

# ---

# Here is the original comparative question:
# "{question}"
# """

#     class AnswerSchema(BaseModel):
#         step_by_step_analysis: str = Field(description="Detailed step-by-step analysis of the answer with at least 5 steps and at least 150 words.")

#         reasoning_summary: str = Field(description="Concise summary of the step-by-step reasoning process. Around 50 words.")

#         relevant_pages: List[int] = Field(description="Just leave empty")

#         final_answer: Union[str, Literal["N/A"]] = Field(description="""
# Company name should be extracted exactly as it appears in question.
# Answer should be either a single company name or 'N/A' if no company is applicable.
# """)

#     pydantic_schema = re.sub(r"^ {4}", "", inspect.getsource(AnswerSchema), flags=re.MULTILINE)

#     example = r"""
# Example:
# Question:
# "Which of the companies had the lowest total assets in USD at the end of the period listed in the annual report: "CrossFirst Bank", "Sleep Country Canada Holdings Inc.", "Holley Inc.", "PowerFleet, Inc.", "Petra Diamonds"? If data for the company is not available, exclude it from the comparison."

# Answer:
# ```
# {
#   "step_by_step_analysis": "1. The question asks for the company with the lowest total assets in USD.\n2. Gather the total assets in USD for each company from the individual answers: CrossFirst Bank: $6,601,086,000; Holley Inc.: $1,249,642,000; PowerFleet, Inc.: $217,435,000; Petra Diamonds: $1,078,600,000.\n3. Sleep Country Canada Holdings Inc. is excluded because its assets are not reported in USD.\n4. Compare the total assets: PowerFleet, Inc. ($217,435,000) < Petra Diamonds ($1,078,600,000) < Holley Inc. ($1,249,642,000)  < CrossFirst Bank ($6,601,086,000).\n5. Therefore, PowerFleet, Inc. has the lowest total assets in USD.",
#   "reasoning_summary": "The individual answers provided the total assets in USD for each company except Sleep Country Canada Holdings Inc. (excluded due to currency mismatch). Direct comparison shows PowerFleet, Inc. has the lowest total assets.",
#   "relevant_pages": [],
#   "final_answer": "PowerFleet, Inc."
# }
# ```
# """

#     system_prompt = build_system_prompt(instruction, example)
    
#     system_prompt_with_schema = build_system_prompt(instruction, example, pydantic_schema)


# class AnswerSchemaFixPrompt:
#     system_prompt = """
# You are a JSON formatter.
# Your task is to format raw LLM response into a valid JSON object.
# Your answer should always start with '{' and end with '}'
# Your answer should contain only json string, without any preambles, comments, or triple backticks.
# """

#     user_prompt = """
# Here is the system prompt that defines schema of the json object and provides an example of answer with valid schema:
# \"\"\"
# {system_prompt}
# \"\"\"

# ---

# Here is the LLM response that not following the schema and needs to be properly formatted:
# \"\"\"
# {response}
# \"\"\"
# """




# class RerankingPrompt:
#     system_prompt_rerank_single_block = """
# You are a RAG (Retrieval-Augmented Generation) retrievals ranker.

# You will receive a query and retrieved text block related to that query. Your task is to evaluate and score the block based on its relevance to the query provided.

# Instructions:

# 1. Reasoning: 
#    Analyze the block by identifying key information and how it relates to the query. Consider whether the block provides direct answers, partial insights, or background context relevant to the query. Explain your reasoning in a few sentences, referencing specific elements of the block to justify your evaluation. Avoid assumptions—focus solely on the content provided.

# 2. Relevance Score (0 to 1, in increments of 0.1):
#    0 = Completely Irrelevant: The block has no connection or relation to the query.
#    0.1 = Virtually Irrelevant: Only a very slight or vague connection to the query.
#    0.2 = Very Slightly Relevant: Contains an extremely minimal or tangential connection.
#    0.3 = Slightly Relevant: Addresses a very small aspect of the query but lacks substantive detail.
#    0.4 = Somewhat Relevant: Contains partial information that is somewhat related but not comprehensive.
#    0.5 = Moderately Relevant: Addresses the query but with limited or partial relevance.
#    0.6 = Fairly Relevant: Provides relevant information, though lacking depth or specificity.
#    0.7 = Relevant: Clearly relates to the query, offering substantive but not fully comprehensive information.
#    0.8 = Very Relevant: Strongly relates to the query and provides significant information.
#    0.9 = Highly Relevant: Almost completely answers the query with detailed and specific information.
#    1 = Perfectly Relevant: Directly and comprehensively answers the query with all the necessary specific information.

# 3. Additional Guidance:
#    - Objectivity: Evaluate block based only on their content relative to the query.
#    - Clarity: Be clear and concise in your justifications.
#    - No assumptions: Do not infer information beyond what's explicitly stated in the block.
# """

#     system_prompt_rerank_multiple_blocks = """
# You are a RAG (Retrieval-Augmented Generation) retrievals ranker.

# You will receive a query and several retrieved text blocks related to that query. Your task is to evaluate and score each block based on its relevance to the query provided.

# Instructions:

# 1. Reasoning: 
#    Analyze the block by identifying key information and how it relates to the query. Consider whether the block provides direct answers, partial insights, or background context relevant to the query. Explain your reasoning in a few sentences, referencing specific elements of the block to justify your evaluation. Avoid assumptions—focus solely on the content provided.

# 2. Relevance Score (0 to 1, in increments of 0.1):
#    0 = Completely Irrelevant: The block has no connection or relation to the query.
#    0.1 = Virtually Irrelevant: Only a very slight or vague connection to the query.
#    0.2 = Very Slightly Relevant: Contains an extremely minimal or tangential connection.
#    0.3 = Slightly Relevant: Addresses a very small aspect of the query but lacks substantive detail.
#    0.4 = Somewhat Relevant: Contains partial information that is somewhat related but not comprehensive.
#    0.5 = Moderately Relevant: Addresses the query but with limited or partial relevance.
#    0.6 = Fairly Relevant: Provides relevant information, though lacking depth or specificity.
#    0.7 = Relevant: Clearly relates to the query, offering substantive but not fully comprehensive information.
#    0.8 = Very Relevant: Strongly relates to the query and provides significant information.
#    0.9 = Highly Relevant: Almost completely answers the query with detailed and specific information.
#    1 = Perfectly Relevant: Directly and comprehensively answers the query with all the necessary specific information.

# 3. Additional Guidance:
#    - Objectivity: Evaluate blocks based only on their content relative to the query.
#    - Clarity: Be clear and concise in your justifications.
#    - No assumptions: Do not infer information beyond what's explicitly stated in the block.
# """

# class RetrievalRankingSingleBlock(BaseModel):
#     """Rank retrieved text block relevance to a query."""
#     reasoning: str = Field(description="Analysis of the block, identifying key information and how it relates to the query")
#     relevance_score: float = Field(description="Relevance score from 0 to 1, where 0 is Completely Irrelevant and 1 is Perfectly Relevant")

# class RetrievalRankingMultipleBlocks(BaseModel):
#     """Rank retrieved multiple text blocks relevance to a query."""
#     block_rankings: List[RetrievalRankingSingleBlock] = Field(
#         description="A list of text blocks and their associated relevance scores."
#     )


# rest of files summary below


# The TextSplitter class processes JSON reports by splitting their text content into smaller chunks, preserving markdown tables and optionally incorporating serialized table data. It uses the langchain library for text splitting and tiktoken for token counting, ensuring chunks are suitable for downstream tasks like vector database ingestion.

# Imports
# json: Handles JSON file reading and writing.
# tiktoken: Counts tokens for text using OpenAI's encoding.
# pathlib: Manages file paths.
# typing: Provides type hints for clarity.
# langchain.text_splitter: Supplies RecursiveCharacterTextSplitter for text chunking.
# Class: TextSplitter
# Purpose: Splits JSON reports into text chunks, optionally including serialized table data.
# _get_serialized_tables_by_page:
# Groups serialized tables by page number.
# Extracts information_block text from each table's serialization.
# Returns a dictionary mapping page numbers to lists of table metadata (page, text, table_id, token count).
# _split_report:
# Splits a report into chunks:
# Loads serialized tables (if provided) and groups them by page.
# Splits each page's text into chunks using _split_page.
# Assigns unique IDs and content type to page chunks.
# Adds serialized table chunks with serialized_table type if available.
# Updates the report's content with the chunk list.
# count_tokens:
# Counts tokens in a string using a specified encoding (default: o200k_base).
# _split_page:
# Splits a page's text into chunks using RecursiveCharacterTextSplitter.
# Configures splitter with gpt-4o model, specified chunk size (default: 300 tokens), and overlap (default: 50 tokens).
# Returns a list of chunk dictionaries with page number, token count, and text.
# split_all_reports:
# Processes all JSON reports in a directory:
# Iterates through JSON files in all_report_dir.
# Matches each report to its serialized table file (if provided).
# Splits the report using _split_report.
# Saves the updated report to output_dir.
# Logs warnings for missing serialized table files.
# Prints the number of processed files.
# Key Features
# Text Chunking: Splits report pages into manageable chunks using langchain with customizable size and overlap.
# Table Integration: Optionally includes serialized table data as separate chunks.
# Token Awareness: Uses tiktoken to track chunk sizes in tokens, compatible with OpenAI models.
# Robust I/O: Handles JSON file reading/writing with proper encoding and directory creation.
# Error Handling: Logs warnings for missing serialized table files.
    
#     The TableSerializer class processes tables in JSON reports, serializing them into context-independent information blocks using OpenAI's API. It supports both synchronous and asynchronous processing, with parallel file handling via threading. The TableSerialization class defines the system prompt and Pydantic models for the serialization output. The code includes custom logging with tqdm integration for progress tracking.

# Imports
# os, json: File and JSON handling.
# asyncio: Asynchronous processing support.
# pathlib: Path management.
# dotenv: Environment variable loading.
# typing: Type hints for clarity.
# pydantic: Data validation and modeling.
# openai: OpenAI client for API requests.
# src.api_requests: Custom OpenAI processors (BaseOpenaiProcessor, AsyncOpenaiProcessor).
# tiktoken: Token counting for OpenAI inputs/outputs.
# tqdm: Progress bar display.
# logging, threading, concurrent.futures, queue, time: Logging, threading, parallel processing, and timing utilities.
# Global Components
# message_queue: A Queue for storing log messages to integrate with tqdm.
# TqdmLoggingHandler: Custom logging handler that sends messages to message_queue.
# process_messages: Displays queued log messages via tqdm.write.
# Class: TableSerializer
# Purpose: Serializes tables in JSON reports into context-independent blocks using OpenAI.
# __init__:
# Inherits from BaseOpenaiProcessor.
# Initializes with preserve_temp_files flag (default: True).
# Creates ./temp directory for temporary files.
# Configures a logger with TqdmLoggingHandler for progress-aware logging.
# _get_table_context:
# Extracts text before and after a table on its page.
# Identifies the table's position relative to other tables or page boundaries.
# Limits context to 3 blocks after the table if no subsequent table exists.
# Returns context as strings.
# _send_serialization_request:
# Constructs a prompt with table HTML and optional context.
# Sends a structured request to OpenAI using gpt-4o-mini-2024-07-18.
# Counts input/output tokens for logging.
# Returns the serialization result.
# _serialize_table:
# Serializes a single table by fetching its context and HTML content.
# Calls _send_serialization_request to process the table.
# serialize_tables:
# Serializes all tables in a JSON report synchronously.
# Adds serialization results to each table's dictionary.
# async_serialize_tables:
# Serializes all tables asynchronously using AsyncOpenaiProcessor.
# Constructs queries for each table with context.
# Updates the JSON report with results, preserving other table attributes.
# process_file:
# Processes a single JSON file:
# Loads the JSON report.
# Runs async_serialize_tables in a new event loop.
# Saves the updated report.
# Cleans up temporary files unless preserve_temp_files is True.
# Handles JSON and other errors with logging.
# process_directory_parallel:
# Processes all JSON files in a directory using a thread pool.
# Displays progress with tqdm.
# Processes log messages periodically to avoid clogging.
# Logs completion status.
# Class: TableSerialization
# Purpose: Defines the system prompt and Pydantic models for table serialization output.
# system_prompt: Instructs the LLM to create context-independent information blocks.
# SerializedInformationBlock (Pydantic model):
# Defines a single block with:
# subject_core_entity: Primary focus (e.g., row header).
# information_block: Detailed, context-rich information including headers, units, and metadata.
# TableBlocksCollection (Pydantic model):
# Defines the collection of blocks with:
# subject_core_entities_list: List of core entities (e.g., row headers).
# relevant_headers_list: List of relevant headers (e.g., column headers).
# information_blocks: List of SerializedInformationBlock instances.
# Key Features
# Table Serialization: Converts HTML tables into context-independent text blocks using OpenAI.
# Context Awareness: Includes surrounding text to enrich table understanding.
# Asynchronous Processing: Supports async API calls for efficiency.
# Parallel Processing: Uses threading for parallel file processing.
# Progress Tracking: Integrates tqdm with custom logging for real-time feedback.
# Error Handling: Logs JSON and processing errors with detailed messages.
# Structured Output: Uses Pydantic models for consistent, validated serialization results.
    


# The VectorRetriever class retrieves relevant document chunks from a collection of processed PDFs using FAISS vector databases and OpenAI embeddings. This simplified version focuses on general query-based retrieval across all documents, removing company-specific and hybrid retrieval functionalities from the original code.

# Imports
# json: Handles JSON document loading.
# logging: Configures debug logging.
# typing: Provides type hints for clarity.
# pathlib: Manages file paths.
# faiss: Handles vector similarity search.
# openai: Provides OpenAI client for embeddings.
# dotenv, os: Load environment variables for API keys.
# numpy: Manages embedding arrays for FAISS.
# Class: VectorRetriever
# Purpose: Retrieves top N relevant document chunks based on a query using vector similarity search.
# __init__:
# Initializes with:
# vector_db_dir: Directory containing FAISS vector databases.
# documents_dir: Directory containing chunked JSON documents.
# Loads vector databases and documents into all_dbs.
# Sets up OpenAI client for embeddings.
# _set_up_llm:
# Configures OpenAI client with API key, no timeout, and 2 retries.
# _load_dbs:
# Loads FAISS vector databases and corresponding JSON documents.
# Matches documents to vector databases by filename stem.
# Logs warnings for missing vector databases and errors for invalid files.
# Returns a list of reports, each containing the report name, vector database, and document data.
# retrieve:
# Retrieves top N chunks across all loaded documents:
# Generates an embedding for the query using OpenAI's text-embedding-3-large model.
# Searches each report's FAISS index for the top N closest chunks.
# Collects results with source, distance, page, and text.
# Sorts by distance (ascending, as lower is better) and returns the top N.
# Raises an error if no databases are loaded.
# Key Features
# Vector Search: Uses FAISS for efficient similarity search with OpenAI embeddings.
# General Retrieval: Retrieves chunks from all processed PDFs without company-specific filtering.
# Error Handling: Logs warnings and errors for missing or invalid files.
# Scalability: Processes multiple reports independently, aggregating results.
# Differences from Original Code
# Removed Classes:
# BM25Retriever: Removed BM25-based retrieval functionality.
# HybridRetriever: Removed hybrid retrieval with LLM reranking.
# Removed Methods:
# retrieve_by_company_name: Company-specific chunk retrieval.
# retrieve_all: Full-page retrieval for a company.
# get_strings_cosine_similarity: String similarity calculation.
# set_up_llm (static): Redundant static LLM setup.
# Simplified Retrieval:
# Focuses on general query-based retrieval across all documents.
# No support for parent page retrieval or LLM reranking.
# Reduced Dependencies:
# Removed rank_bm25, pickle, LLMReranker.
# Output Format:
# Returns source (report name) instead of company-specific metadata.



# The code defines two classes, JinaReranker and LLMReranker, for reranking documents based on their relevance to a query. JinaReranker uses Jina AI's API, while LLMReranker uses an OpenAI LLM to score document relevance, supporting both single and batch document ranking with parallel processing.

# Imports
# os, dotenv: Load environment variables for API keys.
# openai: Provides OpenAI client for LLM interactions.
# requests: Handles HTTP requests to Jina AI's API.
# src.prompts: Custom module for reranking prompts and schemas.
# concurrent.futures: Enables parallel processing with threads.
# Class: JinaReranker
# Purpose: Reranks documents using Jina AI's reranking API.
# __init__:
# Sets the API URL and initializes headers with the Jina API key.
# get_headers:
# Loads the Jina API key from the environment.
# Returns headers with content type and authorization.
# rerank:
# Sends a POST request to Jina's reranking API with the query, documents, and top_n parameter.
# Returns the JSON response with reranked documents.
# Class: LLMReranker
# Purpose: Reranks documents using an OpenAI LLM, combining LLM relevance scores with vector similarity scores.
# __init__:
# Initializes the OpenAI client and loads reranking prompts and schemas from src.prompts.
# set_up_llm:
# Configures the OpenAI client with the API key from the environment.
# get_rank_for_single_block:
# Ranks a single document's relevance using the LLM.
# Formats the query and document into a prompt and sends it to the LLM with a specific schema.
# Returns a dictionary with the relevance score and reasoning.
# get_rank_for_multiple_blocks:
# Ranks multiple documents' relevance in one LLM call.
# Formats documents into a single prompt with delimiters and requests rankings for all documents.
# Returns a dictionary with relevance scores and reasoning for each document.
# rerank_documents:
# Reranks documents by combining LLM relevance scores and vector similarity (distance) scores.
# Supports batch processing:
# If documents_batch_size is 1, processes each document individually using get_rank_for_single_block.
# Otherwise, processes documents in batches using get_rank_for_multiple_blocks.
# Uses parallel threading with ThreadPoolExecutor for efficiency.
# Handles missing rankings by assigning default scores and logging warnings.
# Computes a combined score using llm_weight for LLM scores and 1 - llm_weight for vector distances.
# Sorts documents by combined score in descending order.
# Key Features
# Dual Reranking:
# JinaReranker: Leverages Jina AI's API for fast, external reranking.
# LLMReranker: Uses OpenAI LLM for detailed relevance scoring with customizable prompts.
# Parallel Processing: LLMReranker uses threads to process document batches concurrently.
# Weighted Scoring: Combines LLM relevance and vector similarity scores for robust ranking.
# Error Handling: Logs warnings for missing rankings and assigns default scores.
# Flexible Batching: Supports single-document or batch ranking based on documents_batch_size.


# The QuestionsProcessor class handles question answering using a Retrieval-Augmented Generation (RAG) approach. It retrieves relevant document chunks from a vector database, formats them into a context string, and uses an API-based language model to generate answers. This version is significantly simplified compared to the original, focusing on core functionality without features like company-specific retrieval, parallel processing, or submission formatting.

# Imports
# pathlib: Handles file paths.
# logging: Configures debug logging.
# src.retrieval: Provides VectorRetriever for document retrieval.
# src.api_requests: Provides APIProcessor for interacting with the language model API.
# Class: QuestionsProcessor
# Purpose: Processes questions by retrieving relevant document chunks and generating answers via an API.
# __init__:
# Initializes with:
# vector_db_dir: Directory containing vector databases (FAISS indices).
# documents_dir: Directory containing chunked documents.
# top_n_retrieval: Number of top chunks to retrieve (default: 10).
# api_provider: API provider (default: "openai").
# answering_model: Model name (default: "gpt-4o-2024-08-06").
# Creates instances of VectorRetriever and APIProcessor.
# Logs initialization.
# process_question:
# Processes a single question:
# Retrieves top chunks using VectorRetriever.
# Raises an error if no chunks are found.
# Formats retrieved chunks into a RAG context string.
# Uses APIProcessor to generate an answer based on the context, question, and schema.
# Returns the answer dictionary.
# _format_retrieval_results:
# Formats retrieved chunks into a string.
# Each chunk includes the source document, page number, and text, separated by delimiters.
# Key Features
# RAG Pipeline: Combines vector-based retrieval with API-driven answer generation.
# Simplified Design: Focuses on single-question processing without company extraction or comparative analysis.
# Error Handling: Logs warnings and raises errors for missing context.
# Flexible Schema: Supports customizable answer formats via the schema parameter.
# Differences from Original Code
# Removed Features:
# Company-specific retrieval (_extract_companies_from_subset, get_answer_for_company).
# Comparative question processing (process_comparative_question).
# Parallel question processing (process_questions_list, parallel_requests).
# LLM reranking and parent document retrieval.
# Submission formatting (_post_process_submission_answers, _save_progress).
# Page reference validation (_validate_page_references).
# Statistics calculation (_calculate_statistics).
# Question file loading (_load_questions).
# Thread locking for answer details (_lock, _create_answer_detail_ref).
# Error handling with tracebacks (_handle_processing_error).
# Simplified Retrieval: Uses VectorRetriever without hybrid options.
# No Dependencies: Removes json, pandas, tqdm, threading, concurrent.futures, re.
# Focus: Streamlined for answering questions using all processed PDFs without company-specific logic.
    

# The Pipeline class orchestrates a PDF processing pipeline, handling parsing, merging, chunking, and vector database creation for uploaded PDFs. It uses a simplified configuration compared to the original code, focusing on core functionality without question processing or advanced features like BM25 indexing or table serialization.

# Imports
# dataclasses: Provides @dataclass for structured configuration classes.
# pathlib: Handles file paths.
# logging: Configures debug logging.
# pandas: Used in the original code for JSON-to-CSV conversion (not used in the simplified version).
# src modules: Custom modules for PDF parsing (PDFParser), report merging (PageTextPreparation), text splitting (TextSplitter), and vector database ingestion (VectorDBIngestor).
# Class: PipelineConfig
# Purpose: Defines directory paths for pipeline operations.
# __init__:
# Initializes with a root path and optional PDF reports directory name.
# Sets paths for:
# PDF reports directory (uploaded_pdfs).
# Parsed reports (parsed_reports).
# Merged reports (merged_reports).
# Chunked reports (chunked_reports).
# Vector databases (vector_dbs).
# Logs initialization details.
# Class: RunConfig
# Purpose: Stores runtime configuration.
# Attributes:
# chunk_size: Size of text chunks (default: 1000).
# max_workers: Maximum parallel workers (default: 10).
# Class: Pipeline
# Purpose: Manages the PDF processing pipeline.
# __init__:
# Initializes with a root path and optional RunConfig.
# Creates a PipelineConfig instance to manage paths.
# parse_pdf_reports:
# Parses PDFs using PDFParser.
# Supports sequential or parallel parsing.
# In parallel mode, processes PDFs in chunks using multiple workers.
# Saves parsed JSON reports to parsed_reports_path.
# merge_reports:
# Merges parsed reports into a simpler structure using PageTextPreparation.
# Saves results to merged_reports_path.
# chunk_reports:
# Splits merged reports into smaller chunks using TextSplitter.
# Saves chunks to documents_dir.
# create_vector_dbs:
# Creates vector databases from chunked reports using VectorDBIngestor.
# Saves FAISS indices to vector_db_dir.
# process_uploaded_pdfs:
# Runs the full pipeline:
# Parse PDFs.
# Merge reports.
# Chunk reports.
# Create vector databases.
# Logs progress and errors.
# Key Features
# Simplified Pipeline: Focuses on PDF parsing, merging, chunking, and vector database creation.
# Parallel Processing: Supports parallel PDF parsing for efficiency.
# Modular Design: Uses separate classes for each processing step (PDFParser, PageTextPreparation, etc.).
# Error Handling: Logs exceptions and raises them for debugging.
# Logging: Detailed debug logging for tracking pipeline execution.
# Differences from Original Code
# Removed Features:
# JSON-to-CSV conversion (_convert_json_to_csv_if_needed).
# Table serialization (serialize_tables).
# BM25 database creation (create_bm25_db).
# Question processing (process_questions).
# Markdown export (export_reports_to_markdown).
# Docling model downloading (download_docling_models).
# Advanced RunConfig options (e.g., LLM reranking, submission details).
# Simplified Config:
# PipelineConfig only includes essential paths.
# RunConfig only includes chunk size and max workers.
# Focus: Streamlined for processing uploaded PDFs into vector databases.
    

#     This script processes PDF documents using the PDFParser class to convert them into structured JSON reports, leveraging the DoclingParseV2DocumentBackend. The JsonReportProcessor class assembles the parsed data into a report with metadata, content, tables, and pictures. It supports sequential and parallel processing, metadata integration, and debug data saving.

# Imports
# os, time, logging, re, json: Standard utilities for file operations, timing, logging, regex, and JSON handling.
# tabulate: Formats tables as markdown.
# pathlib: Path handling for file operations.
# typing: Type hints for clarity.
# docling modules: Provide PDF parsing backend, document converter, and data models.
# Function: _process_chunk
# Purpose: Processes a chunk of PDFs in a separate process for parallel execution.
# Logic:
# Creates a new PDFParser instance for the process.
# Sets metadata lookup and debug path.
# Calls parse_and_export on the chunk of PDFs.
# Returns a message with the number of PDFs processed.
# Class: PDFParser
# Purpose: Parses PDFs into structured JSON reports.
# __init__:
# Initializes with a backend (DoclingParseV2DocumentBackend), output directory, thread count, and optional CSV metadata path.
# Creates a document converter and loads metadata if provided.
# Sets thread count environment variable if specified.
# _parse_csv_metadata:
# Parses a CSV file to create a dictionary mapping SHA1 hashes to metadata (e.g., company_name).
# Handles both old and new CSV formats for company names.
# Returns an empty dict on error.
# _create_document_converter:
# Configures a DocumentConverter with a PDF pipeline.
# Enables OCR with English language support and accurate table structure extraction.
# convert_documents:
# Converts a list of PDF paths into structured data using the document converter.
# Returns an iterator of conversion results.
# process_documents:
# Processes conversion results, saving successful ones as JSON files.
# Normalizes page sequences, assembles reports using JsonReportProcessor, and logs results.
# Returns counts of successful and failed conversions.
# _normalize_page_sequence:
# Ensures sequential page numbers by filling gaps with empty pages.
# Copies input data and rebuilds the content array.
# parse_and_export:
# Processes PDFs sequentially.
# Converts documents, processes results, and saves JSON files.
# Logs timing and raises an error if any conversions fail.
# parse_and_export_parallel:
# Processes PDFs in parallel using multiple processes.
# Splits PDFs into chunks based on worker count or chunk size.
# Uses ProcessPoolExecutor to process chunks concurrently.
# Logs progress and timing.
# Class: JsonReportProcessor
# Purpose: Assembles structured JSON reports from parsed document data.
# __init__:
# Initializes with a metadata lookup dictionary and optional debug data path.
# assemble_report:
# Builds a report with metadata, content, tables, and pictures.
# Uses normalized data if provided, otherwise exports from the conversion result.
# assemble_metainfo:
# Creates metadata including SHA1 name, counts of pages, texts, tables, pictures, equations, and footnotes.
# Adds company name from metadata lookup if available.
# process_table:
# Placeholder method for table processing (returns a static string).
# debug_data:
# Saves document data as JSON for debugging if a debug path is set.
# expand_groups:
# Resolves group references in body children, adding group metadata to child items.
# _process_text_reference:
# Processes text references into content items with type, text, and optional fields (orig, enumerated, marker).
# assemble_content:
# Builds page content by processing body children.
# Expands group references and organizes texts, tables, and pictures by page.
# Includes group metadata and page dimensions.
# assemble_tables:
# Assembles table data with ID, page number, bounding box, row/column counts, markdown, HTML, and JSON representations.
# _table_to_md:
# Converts table data to markdown using tabulate.
# Uses the first row as headers if available, with fallback for empty or malformed tables.
# assemble_pictures:
# Assembles picture data with ID, page number, bounding box, and associated text children.
# _process_picture_block:
# Extracts text references associated with a picture, processing them into content items.
# Key Features
# PDF Parsing: Uses DoclingParseV2DocumentBackend for advanced PDF parsing with OCR and table structure extraction.
# Parallel Processing: Supports multi-process parallel parsing for efficiency.
# Structured Output: Produces JSON reports with metadata, page content, tables, and pictures.
# Metadata Integration: Incorporates external metadata from CSV files.
# Error Handling: Logs failures and raises errors for unsuccessful conversions.
# Debugging: Saves raw data for debugging if configured.
    


# The PageTextPreparation class processes JSON report files, cleaning and formatting page content into structured text. It handles blocks like headers, paragraphs, tables, and lists, applies cleaning rules, and supports exporting to markdown.

# Imports
# re: Regular expressions for text cleaning.
# typing: Type hints for clarity.
# pathlib: Path handling for file operations.
# json: JSON parsing and saving.
# Class: PageTextPreparation
# Purpose: Cleans and formats report page blocks, handling groups for tables, lists, and footnotes.
# __init__:
# Initializes with flags for using serialized table data and whether to replace markdown with serialized text.
# process_reports:
# Processes reports from a directory or list of paths.
# Loads each JSON file, processes the report, and combines metadata with formatted content.
# Saves results to an output directory if specified.
# Returns a list of processed reports.
# process_report:
# Processes a single report, iterating through pages.
# Prepares and cleans each page's text, tracking corrections.
# Prints correction details if any are made.
# Returns a structured report with pages and a placeholder for chunks.
# prepare_page_text:
# Main method to process a page.
# Retrieves page data, filters blocks, applies formatting rules.
# Trims leading/trailing whitespace and joins blocks with newlines.
# _get_page_data:
# Retrieves the dictionary for a specific page number from report data.
# Returns None if the page is not found.
# _filter_blocks:
# Removes blocks of ignored types (e.g., page_footer, picture).
# Returns the filtered list of blocks.
# _clean_text:
# Cleans text using regex to replace commands (e.g., /zero -> 0), remove glyphs, and fix capital letters.
# Tracks and counts corrections for reporting.
# Returns cleaned text, correction count, and correction pairs.
# _block_ends_with_colon:
# Checks if a block's text ends with a colon for relevant types (text, caption, etc.).
# _apply_formatting_rules:
# Transforms blocks into formatted text.
# Handles page headers (# or ## based on position), section headers (similar logic), and paragraphs (###).
# Groups tables and lists with headers and footnotes.
# Formats other blocks (text, captions, etc.) directly or skips empty ones.
# Raises an error for unknown block types.
# _render_table_group:
# Renders a group of blocks related to a table.
# Includes headers, table markdown, text, and footnotes.
# Joins blocks with newlines and adds padding.
# _render_list_group:
# Renders a group of blocks for a list.
# Formats headers, list items (-), checkboxes ([x] or [ ]), footnotes, and other text.
# Joins blocks with newlines and adds padding.
# _get_table_by_id:
# Retrieves a table's representation by ID.
# Returns markdown or serialized text based on configuration.
# _get_serialized_table_text:
# Converts a table's serialized data to text.
# Falls back to markdown if no serialized data exists.
# Combines markdown and serialized text or uses serialized text alone based on settings.
# export_to_markdown:
# Processes reports and exports them to markdown files.
# Adds page separators and headers for each page.
# Saves files named by SHA1 hash in the output directory.
# Key Features
# Text Cleaning: Uses regex to fix commands, remove glyphs, and adjust formatting.
# Block Formatting: Applies markdown-style rules (e.g., #, ##, ###) for headers and paragraphs.
# Group Handling: Combines related blocks (e.g., table with header and footnotes, list items).
# Table Flexibility: Supports markdown or serialized table data.
# Output Options: Saves processed reports as JSON or markdown files.


# This script provides two classes for indexing reports: BM25Ingestor for text-based BM25 indexing and VectorDBIngestor for vector-based FAISS indexing using OpenAI embeddings. Both process JSON report files, extract text chunks, and save indices for later retrieval.

# Imports
# os, json, pickle: File operations, JSON parsing, and serialization.
# typing: Type hints for clarity.
# pathlib: Path handling for file operations.
# tqdm: Progress bar for processing reports.
# dotenv: Loads environment variables (e.g., API keys).
# openai: OpenAI API client for embeddings.
# rank_bm25: BM25 algorithm for text-based indexing.
# faiss: Library for vector similarity search.
# numpy: Handles numerical arrays for embeddings.
# tenacity: Retry logic for robustness.
# Class: BM25Ingestor
# Purpose: Creates and saves BM25 indices for text-based retrieval.
# __init__: Empty constructor, no initialization needed.
# create_bm25_index:
# Tokenizes text chunks by splitting on whitespace.
# Builds a BM25 index for ranking based on term frequency.
# process_reports:
# Iterates through JSON report files in a directory.
# Loads each report, extracts text chunks from the 'content' section.
# Creates a BM25 index for the chunks.
# Saves the index as a pickle file named by the report's SHA1 hash.
# Uses tqdm for progress tracking.
# Prints the number of reports processed.
# Class: VectorDBIngestor
# Purpose: Creates and saves FAISS indices for vector-based retrieval using OpenAI embeddings.
# __init__: Initializes the OpenAI client.
# _set_up_llm:
# Loads the API key from environment variables.
# Configures the OpenAI client with no timeout and 2 retries.
# _get_embeddings:
# Generates embeddings using OpenAI's 'text-embedding-3-large' model.
# Handles single strings or lists, splitting large lists into chunks of 1024.
# Retries failed requests twice with a 20-second wait.
# Raises an error for empty strings.
# _create_vector_db:
# Converts embeddings to a NumPy array.
# Creates a FAISS index with cosine distance (Inner Product).
# Adds embeddings to the index.
# _process_report:
# Extracts text chunks from a report.
# Generates embeddings for the chunks.
# Builds a FAISS index from the embeddings.
# process_reports:
# Iterates through JSON report files in a directory.
# Loads each report and processes it to create a FAISS index.
# Saves the index to a file named by the report's SHA1 hash.
# Uses tqdm for progress tracking.
# Prints the number of reports processed.
# Key Features
# BM25 Indexing: Fast, text-based ranking for retrieval using term frequency.
# Vector Indexing: Uses OpenAI embeddings for semantic search, stored in FAISS.
# Robustness: Retry logic for embedding requests, error handling for empty inputs.
# File Management: Saves indices with unique names based on SHA1 hashes.
# Progress Tracking: Uses tqdm for visual feedback during processing.


# This script provides a framework for interacting with three AI providers (OpenAI, IBM, Gemini) to process messages, handle structured outputs, and support RAG (Retrieval-Augmented Generation) queries. It includes synchronous and asynchronous processing, error handling, and retry mechanisms.

# Imports
# os, json: File and JSON operations.
# dotenv: Loads environment variables (e.g., API keys).
# typing: Type hints for better code clarity.
# openai: OpenAI API client.
# asyncio: Asynchronous task management.
# src.api_request_parallel_processor: Custom module for parallel API requests.
# openai.lib._parsing: Converts types to response format parameters.
# tiktoken: Token counting for text.
# src.prompts: Custom prompts for various tasks.
# requests: HTTP requests for IBM API.
# json_repair: Fixes malformed JSON.
# pydantic: Validates structured responses.
# google.generativeai: Gemini API client.
# copy: Deep copying for safety.
# tenacity: Retry logic for robustness.
# Class: BaseOpenaiProcessor
# Purpose: Handles OpenAI API interactions.
# __init__: Initializes the OpenAI client and sets a default model.
# set_up_llm: Configures the OpenAI client with API key, no timeout, and 2 retries.
# send_message: Sends a message to the OpenAI API.
# Supports structured (parsed) or unstructured responses.
# Stores response metadata (model, token counts) and prints it.
# count_tokens: Counts tokens in a string using a specified encoding (e.g., 'o200k_base').
# Class: BaseIBMAPIProcessor
# Purpose: Manages IBM API interactions.
# __init__: Loads API key and sets base URL and default model.
# check_balance: Queries the IBM API for account balance.
# get_available_models: Retrieves available foundation models.
# get_embeddings: Generates vector embeddings for text inputs.
# send_message: Sends a message to the IBM API.
# Supports structured responses with JSON repair and validation.
# Falls back to reparsing if structured parsing fails.
# Stores and prints response metadata.
# _reparse_response: Uses the model to fix invalid responses with a custom prompt.
# Class: BaseGeminiProcessor
# Purpose: Handles Gemini API interactions.
# __init__: Initializes the Gemini client and sets a default model.
# _set_up_llm: Configures the Gemini client with an API key.
# list_available_models: Lists Gemini models supporting text generation.
# _generate_with_retry: Generates content with retry logic (3 attempts, 20-second wait).
# _parse_structured_response: Parses and validates structured responses, falling back to reparsing on error.
# _reparse_response: Fixes invalid JSON responses using the model.
# send_message: Sends a message to the Gemini API.
# Combines system and user prompts.
# Handles structured or unstructured responses.
# Stores and prints response metadata.
# Class: APIProcessor
# Purpose: Routes requests to the appropriate provider (OpenAI, IBM, or Gemini).
# __init__: Initializes the selected processor based on the provider.
# send_message: Forwards message requests to the chosen processor.
# get_answer_from_rag_context: Answers a question using RAG context.
# Builds prompts and response format based on the schema.
# Calls the processor to get a structured answer.
# _build_rag_context_prompts: Constructs system prompt, response format, and user prompt for RAG queries.
# Supports schemas: 'name', 'number', 'boolean', 'names', 'comparative'.
# Adjusts prompts for IBM/Gemini to include schema details.
# get_rephrased_questions: Breaks a comparative question into individual questions per company.
# Class: AsyncOpenaiProcessor
# Purpose: Processes multiple OpenAI requests asynchronously.
# _get_unique_filepath: Generates unique filepaths to avoid overwriting existing files.
# process_structured_ouputs_requests: Handles multiple structured output requests.
# Writes queries to a JSONL file.
# Uses process_api_requests_from_file for parallel processing.
# Monitors progress with a callback.
# Parses and validates results, sorting by original index.
# Optionally preserves or deletes request/results files.
# Returns a list of question-answer pairs.
# Key Features
# Multi-Provider Support: Handles OpenAI, IBM, and Gemini APIs.
# Structured Responses: Uses Pydantic for validation, with JSON repair and reparsing for robustness.
# RAG Support: Processes questions with context using predefined schemas.
# Asynchronous Processing: Efficiently handles multiple OpenAI requests in parallel.
# Error Handling: Includes retries, JSON repair, and fallback mechanisms.
# Token Management: Tracks input/output tokens for all providers.


# This script processes API requests from a file in parallel, respecting rate limits (requests per minute and tokens per minute). It uses asynchronous programming with asyncio and aiohttp to handle multiple requests concurrently, tracks progress, retries failed requests, and saves results or errors to a JSONL file.

# Imports
# aiohttp: Enables asynchronous HTTP requests to the API.
# argparse: Parses command-line arguments (not used in the function but likely for script execution).
# asyncio: Manages asynchronous tasks and concurrency.
# json: Handles JSON serialization for saving results.
# logging: Logs warnings, errors, and debug information.
# os: Provides file system operations (e.g., reading API key, though not directly used here).
# re: Uses regular expressions to extract API endpoints from URLs.
# tiktoken: Counts tokens in requests using a specified encoding.
# time: Manages timing for rate limiting and pauses.
# dataclasses: Defines structured classes (StatusTracker, APIRequest) with minimal boilerplate.
# Main Function: process_api_requests_from_file
# Purpose: Reads API requests from a file, processes them concurrently, and throttles to stay within rate limits.
# Key Steps:
# Setup: Initializes logging, extracts the API endpoint from the URL, and sets up headers (Bearer token or Azure API key).
# Trackers: Uses a queue for retries, a task ID generator, and a StatusTracker to monitor progress.
# Capacity Management: Tracks available request and token capacity, updating them based on time elapsed and rate limits.
# File Reading: Reads JSON requests from a file line by line.
# Request Processing:
# Fetches the next request from the file or retry queue.
# Checks if capacity (requests and tokens) is available.
# If so, makes the API call asynchronously via APIRequest.call_api.
# Rate Limiting: Pauses if a rate limit error occurs, waiting 15 seconds to cool down.
# Loop Control: Sleeps briefly (1 ms) each loop to allow concurrent tasks to run; exits when all tasks are done.
# Logging: Logs completion and any failures or rate limit issues.
# Dataclass: StatusTracker
# Purpose: Tracks the state of API request processing.
# Fields:
# num_tasks_started: Total requests initiated.
# num_tasks_in_progress: Requests currently being processed.
# num_tasks_succeeded: Successfully completed requests.
# num_tasks_failed: Failed requests after all attempts.
# num_rate_limit_errors: Count of rate limit errors.
# num_api_errors: Non-rate-limit API errors.
# num_other_errors: Other exceptions (e.g., network issues).
# time_of_last_rate_limit_error: Timestamp of the last rate limit error for cooldown.
# Dataclass: APIRequest
# Purpose: Represents a single API request with its metadata and a method to call the API.
# Fields:
# task_id: Unique identifier for the request.
# request_json: The JSON payload for the API call.
# token_consumption: Number of tokens the request consumes.
# attempts_left: Remaining retry attempts.
# metadata: Optional metadata from the request.
# result: List to store errors if the request fails.
# Method: call_api:
# Makes an asynchronous POST request using aiohttp.ClientSession.
# Handles responses: if successful, saves the result; if an error occurs, logs it.
# For errors:
# If retries remain, adds the request to the retry queue.
# Otherwise, saves the error and marks the task as failed.
# Updates the status_tracker for success, failure, or error counts.
# Saves results or errors to the specified file in JSONL format.
# Function: api_endpoint_from_url
# Purpose: Extracts the API endpoint path from a URL.
# Logic:
# Uses regex to match standard API URLs (e.g., https://api.example.com/v1/completions -> completions).
# Handles Azure OpenAI URLs (e.g., https://example.com/openai/deployments/model/chat -> chat).
# Returns the matched endpoint path.
# Function: append_to_jsonl
# Purpose: Appends a JSON-serialized payload as a line to a JSONL file.
# Logic:
# Converts the data to a JSON string.
# Opens the file in append mode and writes the string with a newline.
# Function: num_tokens_consumed_from_request
# Purpose: Calculates the number of tokens consumed by a request.
# Logic:
# Uses tiktoken to encode text and count tokens.
# Completions:
# For chat completions (chat/ endpoint): Counts tokens in messages, adding 4 per message (for structure) and 2 for the assistant reply, adjusting for "name" fields.
# For regular completions: Handles single string or list of prompts, adding completion tokens (n * max_tokens).
# Embeddings: Counts tokens in single or multiple inputs.
# Throws errors for unsupported endpoints or invalid input types.
# Function: task_id_generator_function
# Purpose: Generates sequential integer task IDs (0, 1, 2, ...).
# Logic: Uses a generator to yield incrementing IDs indefinitely.
# Key Features
# Asynchronous Processing: Uses asyncio and aiohttp for concurrent API calls, improving throughput.
# Rate Limiting: Tracks requests and tokens per minute, pausing when limits are hit.
# Retry Mechanism: Failed requests are retried up to max_attempts times.
# Error Handling: Captures API errors, rate limit issues, and other exceptions, saving them to the output file.
# Output: Results and errors are saved in JSONL format, including optional metadata.